{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndjson as json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_strip(s):\n",
    "    for i in range(len(s)):\n",
    "        if s[i].islower():\n",
    "            start = i\n",
    "            break\n",
    "    for i in range(len(s))[::-1]:\n",
    "        if s[i].islower():\n",
    "            end = i\n",
    "            break\n",
    "    stripped = s[start:end+1]\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    token = token.lower()\n",
    "    if token.isalpha() or token == \"'s\": return token\n",
    "    if len(''.join(filter(str.isalnum, token))) == 0: return token\n",
    "    if token.isdigit(): return \"N\"\n",
    "    if any(c.isalpha() for c in token): return custom_strip(token)\n",
    "    return \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "with open('poe-sent.txt', 'r') as f:\n",
    "    with open(\"poe.txt\", 'w') as g:\n",
    "        for line in f:\n",
    "            lst = list(map(clean, word_tokenize(line)))\n",
    "            g.write(\" \".join(lst) + '\\n')\n",
    "            all_tokens.extend(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(counter, sz):\n",
    "    mostcommon = [tup[0] for tup in counter.most_common(sz)]\n",
    "    return mostcommon\n",
    "\n",
    "def make_dataset(all_tokens, sz, l):\n",
    "    counter = Counter(all_tokens)\n",
    "    vocab = getvocab(counter, sz)\n",
    "    with open('poe-sent.txt', 'r') as f:\n",
    "        with open('poe-v%d-l%d.txt' % (sz, l), 'w') as g:\n",
    "            lines = f.read().splitlines()\n",
    "            tokens = lines[0].split(\" \")\n",
    "            merged = [token if token in vocab else \"_UNK\" for token in tokens]\n",
    "            for i in range(1, len(lines)):\n",
    "                tokens = lines[i].split(\" \")\n",
    "                tokens = [token if token in vocab else \"_UNK\" for token in tokens]\n",
    "                if len(merged) + len(tokens) <= l:\n",
    "                    merged.extend(tokens)\n",
    "                else:\n",
    "                    if merged.count(\"_UNK\") / len(merged) < 0.15:\n",
    "                        g.write(\" \".join(merged) + '\\n')\n",
    "                    merged = tokens.copy()\n",
    "            if len(merged) > 0 and merged.count(\"_UNK\") / len(merged) < 0.15:\n",
    "                g.write(\" \".join(merged) + '\\n')\n",
    "#             for line in lines:\n",
    "#                 tokens = line.split(\" \")\n",
    "#                 merged = [token if token in vocab else \"_UNK\" for token in tokens]\n",
    "#                 if merged.count(\"_UNK\") / len(merged) < 0.15:\n",
    "#                     g.write(\" \".join(merged) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset(all_tokens, 18000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
